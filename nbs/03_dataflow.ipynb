{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "comic-questionnaire",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "swedish-secretary",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp dataflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "comfortable-drawing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import time\n",
    "\n",
    "from nifi_api.rest import Flowfiles, Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescription-cleaners",
   "metadata": {},
   "source": [
    "# Dataflow\n",
    "\n",
    ">   Monitors and controls a Nifi Dataflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "subjective-lingerie",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "\n",
    "class DataFlow:\n",
    "    \"\"\"\n",
    "    Monitors and controls a Nifi dataflow. The dataflow starts\n",
    "    when the **run** method is called.\n",
    "\n",
    "    Parameters\n",
    "   -------------\n",
    "\n",
    "      dataFlowIds: DataFlowIds\n",
    "        data structure that contains all the IDs of the in/out\n",
    "        processors and connections\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataflow_ids: object,\n",
    "        delay_seconds_after_start: int = 14,\n",
    "        delay_seconds_between_checks: int = 15,\n",
    "    ) -> None:\n",
    "        self.in_processor = Processor(dataflow_ids.in_processor)\n",
    "        self.in_flowfiles = Flowfiles(dataflow_ids.in_connection)\n",
    "        self.middle_processor = Processor(dataflow_ids.middle_processor)\n",
    "        self.out_processor = Processor(dataflow_ids.out_processor)\n",
    "        self.out_flowfiles = Flowfiles(dataflow_ids.out_connection)\n",
    "\n",
    "        self.seconds_after_start = delay_seconds_after_start\n",
    "        self.seconds_between_checks = delay_seconds_between_checks\n",
    "\n",
    "    def run(self) -> None:\n",
    "\n",
    "        print('pipeline watching has started..')\n",
    "\n",
    "        self.out_processor.update_run_status(\"STOPPED\")\n",
    "        self.in_processor.update_run_status(\"RUNNING\")\n",
    "        time.sleep(self.seconds_after_start)\n",
    "        self.in_flowfiles.get_ids()\n",
    "        self.middle_processor.update_run_status(\"RUNNING\")\n",
    "        self.in_processor.update_run_status(\"STOPPED\")\n",
    "\n",
    "        while True:\n",
    "\n",
    "            self.out_flowfiles.get_ids()\n",
    "\n",
    "            if self.in_flowfiles.equals(self.out_flowfiles):\n",
    "\n",
    "                self.middle_processor.update_run_status(\"STOPPED\")\n",
    "                self.out_processor.update_run_status(\"RUNNING\")\n",
    "                print(\"Pipeline watching has finished ...\")\n",
    "                break\n",
    "            time.sleep(self.seconds_between_checks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "convinced-stomach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline watching has started..\n",
      "Pipeline watching has finished ...\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "# Uses the group processor *Test API* in the Cloudera session.\n",
    "\n",
    "# 1. Turn on the  \"Initial\" and \"Middle\" processors, turn off the\n",
    "#    \"Body\" and \"Final\" processors.\n",
    "\n",
    "# 2. Generate the data structure with the connections and processors Ids\n",
    "\n",
    "from nifi_api.environment import DataFlowIds\n",
    "ids = {\n",
    "    \"in_connection\": {\n",
    "        \"Id\": \"cc549c6e-0177-1000-ffff-ffffb5d2aba2\",\n",
    "        \"name\": \"First\"\n",
    "    },\n",
    "    \"out_connection\": {\n",
    "        \"Id\": \"51ab3b24-084f-1309-0000-00001946f2c7\",\n",
    "        \"name\": \"Final\"\n",
    "    },\n",
    "    \"in_processor\": {\n",
    "        \"Id\": \"36c62ad6-d606-3b04-9743-d77b6249608c\",\n",
    "        \"name\": \"First\"\n",
    "    },\n",
    "    \"middle_processor\": {\n",
    "        \"Id\": \"cc54862f-0177-1000-ffff-ffffe7325a20\",\n",
    "        \"name\": \"Middle\"\n",
    "    },\n",
    "    \"out_processor\": {\n",
    "        \"Id\": \"51ab3b1e-084f-1309-a135-aa0100d7186b\",\n",
    "        \"name\": \"Final\"\n",
    "    },\n",
    "}\n",
    "data_ids = DataFlowIds(ids)\n",
    "\n",
    "# 4. Instantiate the DataFlow class as follows:\n",
    "test_dataflow = DataFlow(\n",
    "    dataflow_ids=data_ids,\n",
    "    delay_seconds_after_start=10,\n",
    "    delay_seconds_between_checks=10,\n",
    ")\n",
    "# Call the run method. The following events must happen:\n",
    "#  - \"First\" and \"Last\" proccessor turn on and off, respectively.\n",
    "#  - \"First\" processor turns off and \"Middle\" processor turns on\n",
    "#  - \"Final\" turns on\n",
    "\n",
    "test_dataflow.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "painted-accent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline watching has started..\n",
      "Pipeline watching has finished ...\n"
     ]
    }
   ],
   "source": [
    "# Source To Raw\n",
    "ids = {\n",
    "            \"in_connection\": {\n",
    "                \"type\": \"SuccessConnection\",\n",
    "                \"Id\": \"109133cb-0be0-1603-a259-369b84b4af5d\",\n",
    "                \"description\": \"Set Schemas: Input Port\"\n",
    "            },\n",
    "            \"in_processor\": {\n",
    "                \"type\": \"ListS3Processor\",\n",
    "                \"Id\": \"61a23678-daa8-1e7d-a120-b42561af374d\",\n",
    "                \"description\": \"Lists CSVs in a S3 bucket\"\n",
    "            },\n",
    "            \"middle_processor\": {\n",
    "                \"type\": \"RouteOnAttributeProcessor\",\n",
    "                \"Id\": \"013833bd-24f2-1445-b217-d2646ef11db9\",\n",
    "                \"description\": \"Set Schemas: Filters a list of CSVs\"\n",
    "            },\n",
    "            \"out_connection\": {\n",
    "                \"type\": \"SuccessConnection\",\n",
    "                \"Id\": \"efe531d0-87b8-1e6b-9fe7-e5f950a477bd\",\n",
    "                \"description\": \"Move CSVs ---> Log & Terminate\"\n",
    "            },\n",
    "            \"out_processor\": {\n",
    "                \"type\": \"LogJSONProcessor\",\n",
    "                \"Id\": \"b6223c79-7dfd-1a8c-94f8-6cc3aa5f43b4\",\n",
    "                \"description\": \"Logs JSON\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "data_ids = DataFlowIds(ids)\n",
    "\n",
    "# 4. Instantiate the DataFlow class as follows:\n",
    "test_dataflow = DataFlow(\n",
    "    dataflow_ids=data_ids,\n",
    "    delay_seconds_after_start=10,\n",
    "    delay_seconds_between_checks=10,\n",
    ")\n",
    "# Call the run method. The following events must happen:\n",
    "#  - \"First\" and \"Last\" proccessor turn on and off, respectively.\n",
    "#  - \"First\" processor turns off and \"Middle\" processor turns on\n",
    "#  - \"Final\" turns on\n",
    "\n",
    "test_dataflow.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "subtle-vietnam",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline watching has started..\n",
      "Pipeline watching has finished ...\n"
     ]
    }
   ],
   "source": [
    "# Raw to Discovery\n",
    "ids2 = {\n",
    "            \"in_connection\": {\n",
    "                \"type\": \"SuccessConnection\",\n",
    "                \"Id\": \"b3c83c57-f5ec-15c8-9ae8-934d6a9b01bb\",\n",
    "                \"description\": \"Update Tables: Input Port\"\n",
    "            },\n",
    "            \"in_processor\": {\n",
    "                \"type\": \"GenerateFlowfile\",\n",
    "                \"Id\": \"5d1e3da9-d96a-113d-a78a-56aee3a0ffed\",\n",
    "                \"description\": \"Start Pipeline\"\n",
    "            },\n",
    "            \"middle_processor\": {\n",
    "                \"type\": \"UpdateAttribute\",\n",
    "                \"Id\": \"52333fd3-b8d2-18aa-8d60-b92ac85f8170\",\n",
    "                \"description\": \"Update Tables:Start\"\n",
    "            },\n",
    "            \"out_connection\": {\n",
    "                \"type\": \"SuccessConnection\",\n",
    "                \"Id\": \"46ce3ce0-5f95-1ef0-a749-ab9ca320e4b9\",\n",
    "                \"description\": \"Log & Terminate: Funnel --> Log JSON\"\n",
    "            },\n",
    "            \"out_processor\": {\n",
    "                \"type\": \"AttributeToJSON\",\n",
    "                \"Id\": \"09733dd0-fbd1-1e46-b4d1-130bab4f9414\",\n",
    "                \"description\": \"Logs JSON generates JSON for S3 bucket\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "data_ids2 = DataFlowIds(ids2)\n",
    "\n",
    "# 4. Instantiate the DataFlow class as follows:\n",
    "test_dataflow2 = DataFlow(\n",
    "    dataflow_ids=data_ids2,\n",
    "    delay_seconds_after_start=10,\n",
    "    delay_seconds_between_checks=10,\n",
    ")\n",
    "# Call the run method. The following events must happen:\n",
    "#  - \"First\" and \"Last\" proccessor turn on and off, respectively.\n",
    "#  - \"First\" processor turns off and \"Middle\" processor turns on\n",
    "#  - \"Final\" turns on\n",
    "\n",
    "test_dataflow2.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "organic-repeat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_environment.ipynb.\n",
      "Converted 02_rest.ipynb.\n",
      "Converted 03_dataflow.ipynb.\n",
      "Converted 04_source_to_refined.ipynb.\n",
      "Converted 09_tools.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-killer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
